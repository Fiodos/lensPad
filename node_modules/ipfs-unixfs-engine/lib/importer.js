'use strict';

var _typeof = typeof Symbol === "function" && typeof Symbol.iterator === "symbol" ? function (obj) { return typeof obj; } : function (obj) { return obj && typeof Symbol === "function" && obj.constructor === Symbol ? "symbol" : typeof obj; };

var debug = require('debug');
var log = debug('unixfs');
log.err = debug('unixfs:error');
var fsc = require('./chunker-fixed-size');
var through2 = require('through2');
var merkleDAG = require('ipfs-merkle-dag');
var UnixFS = require('ipfs-unixfs');
var util = require('util');
var bs58 = require('bs58');
var Duplex = require('readable-stream').Duplex;
var isStream = require('isstream');
var streamifier = require('streamifier');

exports = module.exports = Importer;

var CHUNK_SIZE = 262144;

util.inherits(Importer, Duplex);

function Importer(dagService, options) {
  var _this = this;

  Duplex.call(this, { objectMode: true });

  if (!(this instanceof Importer)) {
    return new Importer(dagService);
  }

  if (!dagService) {
    return new Error('must specify a dagService');
  }

  var files = [];
  var counter = 0;

  this._read = function (n) {};

  this._write = function (fl, enc, next) {
    _this.read();
    counter++;
    if (!fl.content) {
      var _ret = function () {
        // 1. create the empty dir dag node
        // 2. write it to the dag store
        // 3. add to the files array {path: <>, hash: <>}
        // 4. emit the path + hash
        var d = new UnixFS('directory');
        var n = new merkleDAG.DAGNode();
        n.data = d.marshal();
        dagService.add(n, function (err) {
          if (err) {
            _this.emit('error', 'Failed to store: ' + fl.path);
            return;
          }
          var el = {
            path: fl.path,
            multihash: n.multihash(),
            size: n.size(),
            dataSize: d.fileSize()
          };
          files.push(el);
          _this.push(el);
          counter--;
          next();
        });
        return {
          v: void 0
        };
      }();

      if ((typeof _ret === 'undefined' ? 'undefined' : _typeof(_ret)) === "object") return _ret.v;
    }

    // Convert a buffer to a readable stream
    if (Buffer.isBuffer(fl.content)) {
      var r = streamifier.createReadStream(fl.content);
      fl.content = r;
    }

    // Bail if 'content' is not readable
    if (!isStream.isReadable(fl.content)) {
      _this.emit('error', new Error('"content" is not a Buffer nor Readable stream'));
      return;
    }

    var leaves = [];
    fl.content.pipe(fsc(CHUNK_SIZE)).pipe(through2(function (chunk, enc, cb) {
      // 1. create the unixfs merkledag node
      // 2. add its hash and size to the leafs array

      // TODO - Support really large files
      // a) check if we already reach max chunks if yes
      // a.1) create a parent node for all of the current leaves
      // b.2) clean up the leaves array and add just the parent node

      var l = new UnixFS('file', chunk);
      var n = new merkleDAG.DAGNode(l.marshal());

      dagService.add(n, function (err) {
        if (err) {
          _this.push({ error: 'Failed to store chunk of: ${fl.path}' });
          return cb(err);
        }

        leaves.push({
          Hash: n.multihash(),
          Size: n.size(),
          leafSize: l.fileSize(),
          Name: ''
        });

        cb();
      });
    }, function (cb) {
      if (leaves.length === 1) {
        // 1. add to the files array {path: <>, hash: <>}
        // 2. emit the path + hash

        var el = {
          path: fl.path,
          multihash: leaves[0].Hash,
          size: leaves[0].Size,
          dataSize: leaves[0].leafSize
        };

        files.push(el);
        _this.push(el);
        return done(cb);
      }
      // 1. create a parent node and add all the leafs
      // 2. add to the files array {path: <>, hash: <>}
      // 3. emit the path + hash of the parent node

      var f = new UnixFS('file');
      var n = new merkleDAG.DAGNode();

      leaves.forEach(function (leaf) {
        f.addBlockSize(leaf.leafSize);
        var l = new merkleDAG.DAGLink(leaf.Name, leaf.Size, leaf.Hash);
        n.addRawLink(l);
      });

      n.data = f.marshal();
      dagService.add(n, function (err) {
        if (err) {
          // this.emit('error', `Failed to store: ${fl.path}`)
          _this.push({ error: 'Failed to store chunk of: ${fl.path}' });
          return cb();
        }

        var el = {
          path: fl.path,
          multihash: n.multihash(),
          size: n.size()
          // dataSize: f.fileSize()
        };

        files.push(el);
        // this.emit('file', el)
        _this.push(el);
        return done(cb);
      });
    }));

    function done(cb) {
      counter--;
      next();
      cb();
    }
  };

  this.end = function () {
    finish.call(_this);

    function finish() {
      var _this2 = this;

      if (counter > 0) {
        return setTimeout(function () {
          finish.call(_this2);
        }, 200);
      }
      // file struct
      // {
      //   path: // full path
      //   multihash: // multihash of the dagNode
      //   size: // cumulative size
      //   dataSize: // dagNode size
      // }

      // 1) convert files to a tree
      // for each path, split, add to a json tree and in the end the name of the
      // file points to an object that is has a key multihash and respective value
      // { foo: { bar: { baz.txt: <multihash> }}}
      // the stop condition is if the value is not an object
      var fileTree = {};
      files.forEach(function (file) {
        var splitted = file.path.split('/');
        if (splitted.length === 1) {
          return; // adding just one file
          // fileTree[file.path] = bs58.encode(file.multihash).toString()
        }
        if (splitted[0] === '') {
          splitted = splitted.slice(1);
        }
        var tmpTree = fileTree;

        for (var i = 0; i < splitted.length; i++) {
          if (!tmpTree[splitted[i]]) {
            tmpTree[splitted[i]] = {};
          }
          if (i === splitted.length - 1) {
            tmpTree[splitted[i]] = file.multihash;
          } else {
            tmpTree = tmpTree[splitted[i]];
          }
        }
      });

      if (Object.keys(fileTree).length === 0) {
        this.push(null);
        return; // no dirs to be created
      }

      // 2) create a index for multihash: { size, dataSize } so
      // that we can fetch these when creating the merkle dag nodes

      var mhIndex = {};

      files.forEach(function (file) {
        mhIndex[bs58.encode(file.multihash)] = {
          size: file.size,
          dataSize: file.dataSize
        };
      });

      // 3) expand leaves recursively
      // create a dirNode
      // Object.keys
      // If the value is an Object
      //   create a dir Node
      //   Object.keys
      //   Once finished, add the result as a link to the dir node
      // If the value is not an object
      //   add as a link to the dirNode

      var pendingWrites = 0;

      function traverse(tree, path, done) {
        var _this3 = this;

        var keys = Object.keys(tree);
        var tmpTree = tree;
        keys.map(function (key) {
          if (_typeof(tmpTree[key]) === 'object' && !Buffer.isBuffer(tmpTree[key])) {
            tmpTree[key] = traverse.call(_this3, tmpTree[key], path ? path + '/' + key : key, done);
          }
        });

        // at this stage, all keys are multihashes
        // create a dir node
        // add all the multihashes as links
        // return this new node multihash

        var d = new UnixFS('directory');
        var n = new merkleDAG.DAGNode();

        keys.forEach(function (key) {
          var b58mh = bs58.encode(tmpTree[key]);
          var l = new merkleDAG.DAGLink(key, mhIndex[b58mh].size, tmpTree[key]);
          n.addRawLink(l);
        });

        n.data = d.marshal();

        pendingWrites++;
        dagService.add(n, function (err) {
          pendingWrites--;
          if (err) {
            _this3.push({ error: 'failed to store dirNode' });
          } else if (path) {
            var el = {
              path: path,
              multihash: n.multihash(),
              yes: 'no',
              size: n.size()
            };
            _this3.push(el);
          }

          if (pendingWrites <= 0) {
            done();
          }
        });

        if (!path) {
          return;
        }

        mhIndex[bs58.encode(n.multihash())] = { size: n.size() };
        return n.multihash();
      }

      var self = this;
      /* const rootHash = */traverse.call(this, fileTree, null, function () {
        self.push(null);
      });
    }
  };
}